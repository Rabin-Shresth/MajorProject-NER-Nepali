{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization of Corpus Data (Nepali News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   IMPORTING NECESSARY PYTHON MODULES\n",
    "\n",
    "import os      # To interact with OS(to list files directory)\n",
    "import csv     #to read and write csv files\n",
    "import nltk    # library\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize #consist function for sentence and word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rabin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DOWNLOADING NLTK RESOURCES\n",
    "\n",
    "nltk.download('punkt')     #tokenizer (uses unsupervised algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion performing tokenization and saving to csv\n",
    "\n",
    "def tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    return sentences, words\n",
    "\n",
    "def saveToCsv(sentences, words, output_csv):\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Sentence', 'Word'])\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for word in word_tokenize(sentence):\n",
    "                csv_writer.writerow([sentence, word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and Output File paths\n",
    "\n",
    "input_directory= 'Nepali_Corpus/Nagarik/politics/'\n",
    "output_csv= 'output_data.csv' #stores the name of CSV file where the tokenized data will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing multiple csv files\n",
    "txt_files = [file for file in os.listdir(input_directory) if file.endswith('.txt')]\n",
    "\n",
    "for txt_file in txt_files:\n",
    "        file_path = os.path.join(input_directory, txt_file)\n",
    "        sentences, words = tokenize_file(file_path)\n",
    "\n",
    "        # Save tokenized data to CSV\n",
    "        saveToCsv(sentences, words, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'txt_fil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m tk\u001b[39m=\u001b[39mword_tokenize(text)\n\u001b[0;32m     13\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(ops, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, newline\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m csvfile:\n\u001b[1;32m---> 14\u001b[0m     csv_writer \u001b[39m=\u001b[39m txt_fil\u001b[39m.\u001b[39mwriter(txt_file)\n\u001b[0;32m     15\u001b[0m     csv_writer\u001b[39m.\u001b[39mwriterow([\u001b[39m'\u001b[39m\u001b[39mWords\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m     csv_writer\u001b[39m.\u001b[39mwriterow([tk])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'txt_fil' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os  \n",
    "import csv\n",
    "from nltk import word_tokenize\n",
    "\n",
    "paths= 'Nepali_Corpus/Nagarik/politics/44747.txt'\n",
    "ops='op.txt'\n",
    "\n",
    "with open(paths, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "tk=word_tokenize(text)\n",
    "with open(ops, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = txt_fil.writer(txt_file)\n",
    "    csv_writer.writerow(['Words'])\n",
    "    csv_writer.writerow([tk])\n",
    "\n",
    "sltext=text.split()\n",
    "print(len(sltext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['जनकपुर-', 'संघीय', 'सरकारले', 'तोकेबाहेक', 'प्रदेश', 'नं', '२', 'मा', 'थप', 'आठ', 'दिन', 'सार्वजनिक', 'बिदा', 'हुने', 'भएको', 'छ।']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os  \n",
    "from nltk import word_tokenize\n",
    "\n",
    "text2='जनकपुर- संघीय सरकारले तोकेबाहेक प्रदेश नं २ मा थप आठ दिन सार्वजनिक बिदा हुने भएको छ।'\n",
    "spltext= text2.split()\n",
    "\n",
    "l=len(spltext)\n",
    "print(spltext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['जनकपुर', 'संघीय', 'सरकारले', 'तोकेबाहेक', 'प्रदेश', 'नं', '२', 'मा', 'थप', 'आठ', 'दिन', 'सार्वजनिक', 'बिदा', 'हुने', 'भएको', 'छ', 'प्रदेश', 'नं', '२', 'मा', 'मनाइने', 'विभिन्न', 'पर्व', 'र', 'दिवसमा', 'सरकारले', 'बिदा', 'दिने', 'निर्णय', 'गरेको', 'छ', 'प्रदेश', 'सरकारको', 'सो', 'निर्णयबमोजिम', 'वैशाख', '११', 'गते', 'जनकपुरमा', 'मनाइने', 'जानकी', 'नवमीको', 'बिदा', 'हुने', 'भएको', 'छ', 'आन्तरिक', 'मामिला', 'तथा', 'कानून', 'मन्त्रालयका', 'प्रशासन', 'महाशाखाका', 'उपसचिव', 'रोहित', 'दाहालका', 'अनुसार', 'छठपर्व', 'र', 'होलीमा', 'दुई–दुई', 'दिन', 'सार्वजनिक', 'बिदा', 'हुनेछ', 'छठपर्व', 'र', 'फागुपूर्णिमामा', 'बिदा', 'दिने', 'संघीय', 'सरकारले', 'निर्णय', 'गरेको', 'छ', 'प्रदेश', 'सरकारले', 'दुवै', 'पर्वको', 'अघिल्लो', 'दिन', 'पनि', 'बिदा', 'दिने', 'निर्णय', 'गरेको', 'दाहालले', 'बताए', 'जानकी', 'नवमी', 'जनैपूर्णिमा', 'विवाहपञ्चमी', 'र', 'रामनवमीमा', 'बिदा', 'दिइनेछ', 'मधेश', 'आन्दोलन', 'स्मृति', 'दिवस', 'माघ', '५', 'गते', 'पनि', 'सार्वजनिक', 'बिदा', 'दिने', 'निर्णय', 'भएको', 'छ', 'आन्तरिक', 'मामिला', 'तथा', 'कानून', 'मन्त्रालयका', 'अनुसार', 'इस्लाम', 'धर्मावलम्बीले', 'मनाउने', 'पैगम्बर', 'हजरत', 'मोहम्मद', 'सल्लाहु', 'वलैहे', 'वसमको', 'जन्मदिन', 'वारह', 'रविउल्ल', 'औयलको', 'दिन', 'पनि', 'बिदा', 'दिइने', 'बताइएको', 'छ', 'प्रदेश', 'सरकारले', 'थप', 'आठ', 'दिन', 'सार्वजनिक', 'बिदा', 'दिने', 'निर्णय', 'गरेको', 'करीब', 'दुई', 'साता', 'बितिसक्दा', 'पनि', 'संघीय', 'सरकार', 'मातहतको', 'मुद्रण', 'विभागबाट', 'प्रकाशन', 'भएको', 'छैन', 'कुनै', 'पनि', 'कानून', 'वा', 'निर्णय', 'मुद्रण', 'विभागमा', 'प्रकाशित', 'गरेपछि', 'मात्र', 'कार्यान्वयनमा', 'आउने', 'प्रावधान', 'रहेको', 'आन्तरिक', 'मामिला', 'मन्त्रालयले', 'जनाएको', 'छ', 'प्रदेश', 'सरकारसँग', 'आफ्नै', 'मुद्रण', 'विभाग', 'नहुँदा', 'संघीय', 'सरकारको', 'सूचना', 'तथा', 'सञ्चार', 'मन्त्रालयअन्तर्गत', 'रहेको', 'विभागमा', 'प्रकाशित', 'गर्नुपर्छ', 'प्रदेश', 'सरकारले', 'प्रदेशस्तरीय', 'सार्वजनिक', 'बिदा', 'दिने', 'निर्णय', 'काठमाडौँ', 'पठाएको', 'एक', 'साता', 'भइसकेको', 'आन्तरिक', 'मामिला', 'मन्त्रालय', 'स्रोतले', 'जनाएको', 'छ', 'रासस']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os  \n",
    "import csv\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "text2='Nepali_Corpus/Nagarik/politics/44747.txt'\n",
    "# txt_files = [file for file in os.listdir('Nepali_Corpus/Nagarik/politics/44747.txt') if file.endswith('.txt')]\n",
    "with open(text2, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "\n",
    "punctuation = ['।',',','-','(',')']\n",
    "\n",
    "for p in punctuation:\n",
    "        text=text.replace(p,'')\n",
    "\n",
    "spltext= word_tokenize(text)\n",
    "\n",
    "\n",
    "print(spltext)\n",
    "\n",
    "out='opt.csv'\n",
    "\n",
    "with open(out, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Words'])\n",
    "    for item in spltext:\n",
    "        csv_writer.writerow([item])\n",
    "\n",
    "\n",
    "# print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for multiple txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "import os\n",
    "import glob  \n",
    "import csv\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "folderPath='Nepali_Corpus/Nagarik/politics'\n",
    "\n",
    "fileList = glob.glob(os.path.join(folderPath,'*.txt'))\n",
    "\n",
    "text=''\n",
    "\n",
    "for filePath in fileList:\n",
    "    with open(filePath, 'r', encoding='utf-8') as file:\n",
    "         text += file.read()  #accumulating contents of all files: sabbai file ko content jodeko\n",
    "\n",
    "\n",
    "punctuation = ['।',',','-','(',')']\n",
    "\n",
    "for p in punctuation:\n",
    "    text=text.replace(p,'')\n",
    "\n",
    "spltext= word_tokenize(text)\n",
    "\n",
    "\n",
    "# print(spltext)\n",
    "\n",
    "out='politics_opt.csv'\n",
    "\n",
    "with open(out, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Words'])\n",
    "    for item in spltext:\n",
    "        csv_writer.writerow([item])\n",
    "\n",
    "\n",
    "# print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob  \n",
    "import csv\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "## Function to read corpus data\n",
    "def read_corpus(folderPath):\n",
    "    fileList = glob.glob(os.path.join(folderPath,'*.txt'))\n",
    "    text=''\n",
    "    for filePath in fileList:\n",
    "        with open(filePath, 'r', encoding='utf-8') as file:\n",
    "            text += file.read()  #accumulating contents of all files: sabbai file ko content jodeko\n",
    "    return text\n",
    "\n",
    "## Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuation = ['।',',','-','(',')','.','’','‘']\n",
    "    for p in punctuation:\n",
    "        text=text.replace(p,'')\n",
    "    return text\n",
    "\n",
    "## Function to remove stopwords\n",
    "def remove_stopwords(stopwords, text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "## Function to tokenize\n",
    "def tokenize():\n",
    "    folderPath='Nepali_Corpus/Corpus'\n",
    "    stopWords = './stopword1.txt'\n",
    "    text=read_corpus(folderPath)\n",
    "    text=remove_punctuation(text)\n",
    "    with open(stopWords, 'r', encoding='utf-8') as file:\n",
    "        stopwords = file.read()\n",
    "    text=remove_stopwords(stopwords, text)\n",
    "    spltext= word_tokenize(text)\n",
    "    return spltext\n",
    "\n",
    "## Function to write tokenized data to csv file\n",
    "def write_to_csv(spltext):\n",
    "    out='properTest.csv'\n",
    "    with open(out, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Words'])\n",
    "        for item in spltext:\n",
    "            csv_writer.writerow([item])\n",
    "\n",
    "## Main function\n",
    "def main():\n",
    "    spltext=tokenize()\n",
    "    write_to_csv(spltext)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Framing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
